# Best of AI Agent - Comprehensive Documentation

## Table of Contents
- [Overview & Introduction](#overview--introduction)
- [Quick Start Guide](#quick-start-guide)
- [Core Features](#core-features)
- [User Interface Guide](#user-interface-guide)
- [Advanced Features](#advanced-features)
- [Technical Architecture](#technical-architecture)
- [Configuration & Setup](#configuration--setup)
- [Troubleshooting & Support](#troubleshooting--support)
- [API Reference](#api-reference)
- [Development & Contributing](#development--contributing)

---

## Overview & Introduction

### What is Best of AI Agent?

**Best of AI Agent** is a sophisticated article extraction and processing application that transforms URLs and web content into professional newspaper-style clippings and documents. Built with Python and Streamlit, it provides a comprehensive solution for content aggregation, processing, and presentation.

### Key Capabilities at a Glance

üîç **Smart Article Extraction**
- Extract articles from any web URL with intelligent content detection
- Support for complex websites with advanced scraping techniques
- Automatic content cleaning and formatting

üìÑ **Document Processing** 
- Upload Word documents (.docx) containing multiple URLs
- Automatic URL extraction from text, hyperlinks, and tables
- Batch processing of hundreds of articles simultaneously

üì∞ **Professional Newspaper Generation**
- Create stunning newspaper-style clippings from articles
- Multiple layout options (single, double, triple column)
- Professional typography and formatting
- Automatic image integration and layout optimization

‚òÅÔ∏è **Cloud Integration**
- Google Drive integration for document storage and sharing
- Replit Object Storage for scalable file management
- Automatic backup and synchronization

üîê **Advanced Authentication**
- Secure Google Drive OAuth integration
- Newspapers.com premium account support
- Credential management and session handling

‚ö° **Batch Processing Engine**
- Concurrent processing of multiple URLs (configurable workers)
- Intelligent rate limiting and retry mechanisms
- Real-time progress tracking and error reporting

### Target Use Cases

- **Research & Academic Work**: Aggregate and format research articles
- **Content Curation**: Create professional article collections
- **News Monitoring**: Process multiple news sources into readable formats
- **Archive Creation**: Convert web content into permanent documents
- **Publication Preparation**: Format articles for newsletters or reports

---

## Quick Start Guide

### Prerequisites

- Python 3.12 or higher
- Modern web browser (Chrome/Firefox recommended)
- Internet connection for web scraping
- Optional: Google Drive account for cloud storage
- Optional: Newspapers.com account for premium features

### Installation

1. **Clone or Download the Application**
   ```bash
   git clone <repository-url>
   cd best-of-ai-agent
   ```

2. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   # or using uv (recommended)
   uv pip install -r pyproject.toml
   ```

3. **Launch the Application**
   ```bash
   streamlit run app.py
   ```

4. **Access the Web Interface**
   - Open your browser to `http://localhost:8501`
   - The application will automatically load with the main interface

### First-Time Setup (5 Minutes)

1. **Upload a Sample Document**
   - Create a Word document with some URLs
   - Use the "Process Documents" tab to upload it
   - Review extracted URLs in the data table

2. **Configure Basic Settings**
   - Set concurrent workers (start with 2-3)
   - Configure request delays (500ms recommended)
   - Choose output formats

3. **Test Article Processing**
   - Select a few URLs from your document
   - Click "Start Batch Processing"
   - Monitor progress and review results

4. **Generate Newspaper Clippings**
   - Navigate to the "Results" tab
   - Use the newspaper conversion tools
   - Download generated images and documents

---

## Core Features

### 1. Document Processing & URL Extraction

#### Supported Document Formats
- **Word Documents (.docx)**: Full support for modern Word formats
- **Text Content**: URLs from paragraph text
- **Hyperlinks**: Clickable links with automatic extraction
- **Tables**: URLs within table cells
- **Mixed Content**: Combination of text and hyperlinked URLs

#### URL Detection & Validation
- **Smart Pattern Recognition**: Detects URLs even without http/https prefixes
- **Format Validation**: Ensures URLs are properly formatted and accessible
- **Duplicate Removal**: Automatically removes duplicate URLs
- **Domain Filtering**: Option to filter by specific domains or patterns

#### Processing Pipeline
```
Document Upload ‚Üí URL Extraction ‚Üí Validation ‚Üí Deduplication ‚Üí Preview ‚Üí Processing
```

### 2. Article Extraction Engine

#### Content Extraction
- **Intelligent Content Detection**: Uses multiple algorithms to identify main article content
- **Metadata Extraction**: Automatically extracts titles, authors, dates, and descriptions
- **Image Detection**: Identifies and downloads relevant images
- **Content Cleaning**: Removes ads, navigation, and irrelevant content

#### Supported Website Types
- **News Websites**: CNN, BBC, Reuters, local news sites
- **Blog Platforms**: WordPress, Medium, Substack
- **Academic Sources**: Research papers, institutional sites
- **General Websites**: Most HTML-based content sites

#### Advanced Features
- **JavaScript Rendering**: Handles dynamic content with Selenium
- **Cookie Management**: Maintains session cookies for authenticated sites
- **Rate Limiting**: Respectful scraping with configurable delays
- **Error Recovery**: Automatic retries with exponential backoff

### 3. Newspaper Clipping Generation

#### Layout Engine
The application includes a sophisticated newspaper layout engine with multiple options:

**Single Column Layout**
- Width: 800px, Height: 1200px
- Ideal for: Long-form articles, detailed content
- Typography: Large headlines, readable body text

**Two Column Layout**  
- Width: 1000px, Height: 1200px
- Column Gap: 40px
- Ideal for: Standard news articles, balanced content

**Three Column Layout**
- Width: 1200px, Height: 1200px  
- Column Gap: 30px
- Ideal for: Short articles, news briefs

#### Typography & Design
- **Professional Fonts**: Carefully selected font families for newspaper aesthetic
- **Hierarchical Text**: Headlines, subheadings, body text with proper sizing
- **Smart Text Wrapping**: Intelligent line breaks and paragraph formatting
- **Image Integration**: Automatic image placement and sizing
- **Border Effects**: Decorative elements for authentic newspaper look

#### Output Formats
- **PNG Images**: High-resolution clippings for sharing
- **Word Documents**: Editable documents with embedded images
- **Markdown Files**: Plain text format for further processing
- **ZIP Archives**: Bulk download of all generated content

### 4. Batch Processing System

#### Concurrent Processing
- **Worker Pool Management**: Configurable number of concurrent workers (1-10)
- **Load Balancing**: Intelligent distribution of URLs across workers
- **Resource Management**: Memory and CPU usage optimization
- **Progress Tracking**: Real-time updates on processing status

#### Queue Management
- **Priority Queuing**: Process important URLs first
- **Error Handling**: Failed URLs automatically retry with exponential backoff
- **Rate Limiting**: Configurable delays between requests (100ms-5000ms)
- **Timeout Management**: Prevents hanging on unresponsive websites

#### Performance Monitoring
- **Real-time Metrics**: Success/failure rates, processing speed
- **Detailed Logging**: Comprehensive logs for debugging
- **Resource Usage**: Monitor memory and CPU consumption
- **Bottleneck Detection**: Identify and resolve performance issues

### 5. Authentication Systems

#### Google Drive Integration
- **OAuth 2.0 Flow**: Secure authentication using Google's official API
- **Automatic Token Refresh**: Seamless re-authentication
- **Scoped Permissions**: Only requests necessary permissions
- **Session Management**: Persistent authentication across sessions

#### Newspapers.com Support
- **Premium Account Integration**: Access to subscriber-only content
- **Cookie-based Authentication**: Maintains login sessions
- **Advanced Search**: Premium search features and historical archives
- **Content Access**: Full article text and high-resolution images

---

## User Interface Guide

### Main Application Layout

The application uses a tabbed interface with three main sections:

#### Tab 1: Process Documents üìÑ
**Purpose**: Upload documents, extract URLs, and configure processing

**Key Components**:
- **File Upload Zone**: Drag-and-drop or click to upload .docx files
- **URL Review Table**: Interactive table showing extracted URLs
- **Manual URL Management**: Add/remove URLs manually
- **Processing Controls**: Start/stop batch processing

**Workflow**:
1. Upload Word document containing URLs
2. Review and edit the extracted URL list
3. Configure processing settings in sidebar
4. Click "Start Batch Processing"
5. Monitor real-time progress

#### Tab 2: Results üìä  
**Purpose**: View processing results and generate outputs

**Key Components**:
- **Processing Summary**: Success/failure statistics
- **Article Previews**: View extracted content
- **Image Gallery**: Browse downloaded images
- **Export Options**: Download in various formats

**Features**:
- **Newspaper Conversion**: Transform articles into newspaper clippings
- **Word Document Generation**: Create formatted Word documents
- **Bulk Download**: ZIP files with all content
- **Cloud Upload**: Send results to Google Drive

#### Tab 3: Advanced ‚öôÔ∏è
**Purpose**: Advanced features and testing

**Key Components**:
- **Single Article Test**: Test extraction on individual URLs
- **Newspapers.com Search**: Search historical newspaper archives
- **Debug Tools**: Advanced debugging and logging options
- **Authentication Management**: Manage cloud service connections

### Sidebar Configuration

#### Processing Settings
- **Concurrent Workers**: Number of simultaneous processing threads (1-10)
- **Request Delay**: Delay between requests in milliseconds (100-5000)
- **Timeout Settings**: Request timeout limits (10-60 seconds)
- **Retry Logic**: Number of retry attempts for failed URLs (1-5)

#### Output Configuration  
- **Image Resolution**: Quality settings for generated images (72-300 DPI)
- **Document Format**: Choose output formats (PNG, DOCX, MD, ZIP)
- **Filename Templates**: Customize output file naming patterns
- **Storage Location**: Local or cloud storage options

#### Authentication Status
- **Google Drive**: Connection status and account information
- **Newspapers.com**: Premium account status and capabilities
- **Credential Management**: View and refresh authentication tokens

---

## Advanced Features

### Newspapers.com Integration

#### Premium Search Capabilities
- **Historical Archive Access**: Search newspapers from 1700s to present
- **Advanced Filters**: Date ranges, locations, publications
- **Full-text Search**: Search within article content
- **Image Downloads**: High-resolution newspaper page images

#### Search Interface
```
Search Query ‚Üí Filter Options ‚Üí Results Preview ‚Üí Selection ‚Üí Download
```

**Search Options**:
- **Keyword Search**: Find articles containing specific terms
- **Date Range**: Limit results to specific time periods  
- **Geographic Filter**: Search by state, city, or region
- **Publication Filter**: Search specific newspaper titles
- **Content Type**: Articles, obituaries, advertisements, etc.

#### Result Processing
- **OCR Text Extraction**: Convert newspaper images to searchable text
- **Metadata Extraction**: Date, publication, page information
- **Content Formatting**: Clean and format extracted text
- **Image Enhancement**: Improve readability of historical images

### Google Drive Integration

#### Authentication Flow
1. **Initial Setup**: Click "Authenticate with Google Drive"
2. **OAuth Consent**: Approve permissions in browser popup
3. **Token Storage**: Secure credential storage for future sessions
4. **Auto-refresh**: Automatic token renewal when expired

#### File Management
- **Folder Organization**: Automatic folder creation by date/project
- **Metadata Tagging**: Add custom metadata to uploaded files
- **Version Control**: Track multiple versions of documents
- **Sharing Controls**: Set permissions for shared documents

#### Bulk Operations
- **Batch Upload**: Upload multiple files simultaneously
- **Progress Tracking**: Real-time upload progress
- **Error Recovery**: Resume interrupted uploads
- **Duplicate Detection**: Avoid uploading duplicate content

### Advanced Processing Options

#### Content Filtering
- **Minimum Length**: Skip articles below specified word count
- **Language Detection**: Process only articles in specified languages
- **Content Type Filtering**: Focus on news, blogs, academic papers, etc.
- **Quality Scoring**: Rate and filter content based on quality metrics

#### Image Processing
- **OCR Capabilities**: Extract text from images using Tesseract
- **Image Enhancement**: Improve quality of downloaded images
- **Format Conversion**: Convert between image formats (PNG, JPG, WebP)
- **Compression**: Optimize file sizes for storage and sharing

#### Export Customization  
- **Template System**: Custom newspaper layout templates
- **Branding Options**: Add logos and custom headers
- **Typography Controls**: Font selection and sizing options
- **Color Schemes**: Choose from predefined color palettes

---

## Technical Architecture

### System Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Streamlit Web Interface                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                   Application Controller                     ‚îÇ
‚îÇ                        (app.py)                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ          Core Processing Modules                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ URL         ‚îÇ ‚îÇ Article     ‚îÇ ‚îÇ Document               ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Extractor   ‚îÇ ‚îÇ Processor   ‚îÇ ‚îÇ Processor              ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ          Utility Modules                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Storage     ‚îÇ ‚îÇ Batch       ‚îÇ ‚îÇ Authentication         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Manager     ‚îÇ ‚îÇ Processor   ‚îÇ ‚îÇ Manager                ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ          External Services                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Google      ‚îÇ ‚îÇ Newspapers  ‚îÇ ‚îÇ Replit Object          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Drive API   ‚îÇ ‚îÇ .com API    ‚îÇ ‚îÇ Storage                ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Core Modules

#### app.py - Application Controller
**Location**: `/app.py`  
**Purpose**: Main application entry point and UI coordination

**Key Functions**:
- `main()`: Application initialization and main UI rendering
- `handle_oauth_callback()`: Manages Google Drive authentication
- `handle_document_upload()`: Processes uploaded documents
- `display_batch_results()`: Shows processing results
- `handle_newspapers_search()`: Manages Newspapers.com integration

#### extractors/ - Content Extraction
**Location**: `/extractors/`  
**Purpose**: Extract content from various sources

**url_extractor.py**:
- `extract_from_url()`: Extract article content from web URLs
- Content cleaning and formatting
- Image download and processing
- Metadata extraction

**newspapers_extractor.py**:
- `NewspapersComExtractor`: Premium Newspapers.com integration
- OCR text extraction from newspaper images
- Historical archive search and retrieval
- Authentication and session management

#### utils/ - Utility Modules
**Location**: `/utils/`  
**Purpose**: Support functions and specialized processing

**Key Modules**:
- `document_processor.py`: Word document handling and URL extraction
- `storage_manager.py`: File storage and cloud integration
- `batch_processor.py`: Concurrent processing coordination
- `google_drive_manager.py`: Google Drive API integration
- `credential_manager.py`: Authentication and credential storage
- `newspaper_converter.py`: Newspaper layout generation
- `logger.py`: Comprehensive logging system

### Data Flow Architecture

#### Processing Pipeline
```
Input ‚Üí Extraction ‚Üí Processing ‚Üí Generation ‚Üí Output
  ‚îÇ         ‚îÇ           ‚îÇ           ‚îÇ          ‚îÇ
  ‚îÇ         ‚îÇ           ‚îÇ           ‚îÇ          ‚îî‚îÄ‚Üí Files/Images
  ‚îÇ         ‚îÇ           ‚îÇ           ‚îî‚îÄ‚Üí Newspaper Clipping
  ‚îÇ         ‚îÇ           ‚îî‚îÄ‚Üí Content Formatting
  ‚îÇ         ‚îî‚îÄ‚Üí URL Extraction/Article Content
  ‚îî‚îÄ‚Üí Document Upload/URL Input
```

#### Detailed Data Flow
1. **Input Stage**: 
   - Document upload via Streamlit file_uploader
   - URL extraction using python-docx library
   - Manual URL input through web interface

2. **Extraction Stage**:
   - Concurrent URL processing using ThreadPoolExecutor
   - Web scraping with requests/selenium-wire
   - Content extraction using BeautifulSoup
   - Image download and processing

3. **Processing Stage**:
   - Content cleaning and formatting
   - Metadata extraction and validation
   - Image optimization and conversion
   - Error handling and retry logic

4. **Generation Stage**:
   - Newspaper layout engine processing
   - Typography and design application
   - Image integration and positioning
   - Multiple format generation (PNG, DOCX, MD)

5. **Output Stage**:
   - Local file storage
   - Cloud upload (Google Drive/Replit Object Storage)
   - ZIP archive generation
   - Progress reporting and completion

### Storage Architecture

#### Multi-tier Storage System
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Cloud Storage (Tier 1)                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Google      ‚îÇ ‚îÇ Replit Object Storage                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Drive       ‚îÇ ‚îÇ (Primary for images/documents)          ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                    Local Storage (Tier 2)                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ local_storage/ (Fallback when cloud unavailable)        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚îú‚îÄ‚îÄ extracted_articles/                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚îú‚îÄ‚îÄ extracted_images/                                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚îú‚îÄ‚îÄ document_capsules/                                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ ‚îî‚îÄ‚îÄ debug/                                             ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                 Temporary Storage (Tier 3)                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ System temp directory (Processing cache)                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Session state (Runtime data)                           ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Configuration & Setup

### Environment Configuration

#### Required Environment Variables
```bash
# Optional: Default storage bucket name
REPLIT_BUCKET_NAME=your-bucket-name

# Optional: Default processing settings
DEFAULT_WORKERS=3
DEFAULT_DELAY=500

# Optional: Authentication settings
GOOGLE_CLIENT_ID=your-google-client-id
GOOGLE_CLIENT_SECRET=your-google-client-secret
```

#### Configuration Files
- **pyproject.toml**: Python dependencies and project metadata
- **replit.nix**: Replit-specific environment configuration
- **enhanced_auth_data.json**: Credential storage (auto-generated)

### Authentication Setup

#### Google Drive Setup
1. **Create Google Cloud Project**:
   - Go to [Google Cloud Console](https://console.cloud.google.com)
   - Create new project or select existing
   - Enable Google Drive API

2. **Create OAuth Credentials**:
   - Go to APIs & Credentials ‚Üí Credentials
   - Create OAuth 2.0 Client ID
   - Add authorized redirect URIs
   - Download credentials JSON

3. **Configure Application**:
   - Place credentials file in project root
   - Use "Authenticate with Google Drive" button in app
   - Complete OAuth flow in browser

#### Newspapers.com Setup
1. **Premium Account Required**:
   - Sign up for Newspapers.com premium subscription
   - Verify account has full access privileges

2. **Browser Authentication**:
   - Use "Load Newspapers.com Credentials" in sidebar
   - Application will extract cookies from browser
   - Automatic session management

### Deployment Options

#### Local Development
```bash
# Clone repository
git clone <repository-url>
cd best-of-ai-agent

# Install dependencies
pip install -r requirements.txt

# Run application
streamlit run app.py
```

#### Replit Deployment
1. **Import Project**: Import GitHub repository to Replit
2. **Install Dependencies**: Replit will auto-install from pyproject.toml
3. **Configure Environment**: Set environment variables in Replit Secrets
4. **Run Application**: Use "Run" button or command `streamlit run app.py`

#### Docker Deployment
```dockerfile
FROM python:3.12-slim

WORKDIR /app
COPY . .

RUN pip install -r requirements.txt

EXPOSE 8501

CMD ["streamlit", "run", "app.py", "--server.address", "0.0.0.0"]
```

#### Cloud Deployment (Heroku, AWS, GCP)
- **Heroku**: Add Procfile with `web: streamlit run app.py --server.port $PORT`
- **AWS**: Use Elastic Beanstalk with Python platform
- **GCP**: Deploy to Cloud Run with container image

---

## Troubleshooting & Support

### Common Issues & Solutions

#### Installation Issues

**Problem**: `ImportError: Unable to import required dependencies: numpy`
```bash
# Solution 1: Clean installation
pip cache purge
pip uninstall numpy pandas
pip install --no-cache-dir numpy==2.1.3 pandas==2.2.3

# Solution 2: Use UV package manager
uv pip uninstall numpy pandas
uv pip install --no-cache numpy pandas
```

**Problem**: `ModuleNotFoundError: No module named 'streamlit'`
```bash
# Solution: Install all dependencies
pip install -r requirements.txt
# or
uv pip install -r pyproject.toml
```

#### Authentication Issues  

**Problem**: Google Drive authentication fails
- **Check**: Credentials file exists and is valid JSON
- **Verify**: OAuth redirect URLs match application settings
- **Solution**: Delete `enhanced_auth_data.json` and re-authenticate

**Problem**: Newspapers.com authentication fails
- **Check**: Premium subscription is active
- **Verify**: Browser cookies are accessible
- **Solution**: Clear browser cache and re-authenticate

#### Processing Issues

**Problem**: URLs not extracting from Word document
- **Check**: Document is .docx format (not .doc)
- **Verify**: URLs are properly formatted with http/https
- **Solution**: Try manual URL entry to test extraction

**Problem**: Articles failing to extract
- **Check**: URLs are accessible (test in browser)
- **Verify**: Websites don't block automated access
- **Solution**: Increase request delays, check for CAPTCHA

#### Performance Issues

**Problem**: Slow processing with many URLs
- **Solution**: Reduce concurrent workers to 2-3
- **Solution**: Increase request delays to 1000ms+
- **Solution**: Process URLs in smaller batches

**Problem**: Memory usage too high
- **Solution**: Reduce image resolution settings
- **Solution**: Enable garbage collection in advanced settings
- **Solution**: Process fewer URLs simultaneously

### Debug Tools

#### Logging System
- **Enable Debug Mode**: Toggle in sidebar for verbose logging
- **Log Files**: Stored in `logs/` directory with timestamps
- **Real-time Logs**: View processing logs in application interface

#### Debug Output
- **Debug HTML**: Saved pages for failed extractions in `debug_html/`
- **Error Summaries**: JSON files with detailed error information
- **Session Data**: View current session state in advanced tab

#### Performance Monitoring
- **Processing Metrics**: Success/failure rates, timing information
- **Resource Usage**: Memory and CPU monitoring
- **Network Activity**: Request/response logging with timing

### Error Recovery

#### Automatic Recovery
- **Retry Logic**: Failed URLs automatically retry with exponential backoff
- **Session Persistence**: Processing state maintained across browser refreshes
- **Graceful Degradation**: Fallback options when cloud services unavailable

#### Manual Recovery
- **Resume Processing**: Continue from where processing stopped
- **Selective Retry**: Retry only failed URLs from previous run
- **Data Recovery**: Restore session data from local storage

### Performance Optimization

#### Processing Speed
- **Optimal Workers**: 3-4 workers for most systems
- **Request Delays**: 500-1000ms for respectful scraping
- **Batch Sizes**: Process 20-50 URLs per batch for best results

#### Memory Usage
- **Image Settings**: Use 150 DPI for balance of quality/size
- **Cleanup**: Regular cleanup of temporary files
- **Session Management**: Clear old session data periodically

#### Network Optimization
- **Timeout Settings**: 30 seconds for most websites
- **Retry Limits**: 3 retries maximum to avoid infinite loops
- **Connection Pooling**: Reuse connections when possible

### Getting Help

#### Documentation Resources
- **Feature Documentation**: Check `feature-docs/` directory
- **Code Comments**: Detailed inline documentation
- **API Reference**: Function and class documentation

#### Support Channels
- **GitHub Issues**: Report bugs and request features
- **Debug Information**: Always include debug logs with issues
- **Version Information**: Include Python and dependency versions

#### Contributing  
- **Bug Reports**: Use issue templates with detailed information
- **Feature Requests**: Describe use case and implementation ideas
- **Code Contributions**: Follow existing code style and patterns

---

## API Reference

### Core Functions

#### Article Extraction

```python
from extractors.url_extractor import extract_from_url

def extract_from_url(url: str, timeout: int = 30) -> dict:
    """
    Extract article content from a URL
    
    Args:
        url: The URL to extract content from
        timeout: Request timeout in seconds
        
    Returns:
        dict: {
            'success': bool,
            'title': str,
            'content': str,
            'author': str,
            'date': str,
            'images': list,
            'error': str (if failed)
        }
    """
```

#### Document Processing

```python
from utils.document_processor import extract_urls_from_docx

def extract_urls_from_docx(file_content) -> List[str]:
    """
    Extract URLs from Word document
    
    Args:
        file_content: Uploaded file content from Streamlit
        
    Returns:
        List[str]: List of unique URLs found in document
    """
```

#### Batch Processing

```python
from utils.batch_processor import BatchProcessor

class BatchProcessor:
    def __init__(self, max_workers: int = 3, delay: float = 0.5):
        """
        Initialize batch processor
        
        Args:
            max_workers: Number of concurrent workers
            delay: Delay between requests in seconds
        """
    
    def process_urls(self, urls: List[str], callback=None) -> dict:
        """
        Process multiple URLs concurrently
        
        Args:
            urls: List of URLs to process
            callback: Optional progress callback function
            
        Returns:
            dict: Processing results with success/failure counts
        """
```

#### Storage Management

```python
from utils.storage_manager import StorageManager

class StorageManager:
    def upload_file(self, file_data: bytes, filename: str, 
                   metadata: dict = None) -> dict:
        """
        Upload file to configured storage backend
        
        Args:
            file_data: File content as bytes
            filename: Desired filename
            metadata: Optional metadata dictionary
            
        Returns:
            dict: Upload result with URL and metadata
        """
```

### Configuration Options

#### Processing Configuration
```python
config = {
    'max_workers': 3,           # Concurrent processing threads
    'request_delay': 0.5,       # Delay between requests (seconds)
    'timeout': 30,              # Request timeout (seconds)
    'max_retries': 3,           # Maximum retry attempts
    'min_content_length': 100,  # Minimum article length (words)
    'image_resolution': 150,    # Image DPI for output
    'enable_debug': False       # Enable debug logging
}
```

#### Output Configuration
```python
output_config = {
    'formats': ['png', 'docx', 'md'],  # Output formats
    'newspaper_layout': 'two_column',   # Layout type
    'include_images': True,             # Include images in output
    'compress_images': True,            # Compress images for size
    'filename_template': '{date}_{title}', # Filename pattern
    'storage_backend': 'replit'         # Storage system to use
}
```

### Extension Points

#### Custom Extractors
```python
class CustomExtractor:
    def extract(self, url: str) -> dict:
        """Custom extraction logic"""
        # Implement custom extraction logic
        return {
            'success': True,
            'title': 'Article Title',
            'content': 'Article content...',
            'metadata': {}
        }

# Register custom extractor
from extractors import register_extractor
register_extractor('custom', CustomExtractor())
```

#### Custom Storage Backends
```python
class CustomStorageBackend:
    def upload(self, file_data: bytes, filename: str) -> str:
        """Upload file and return URL"""
        # Implement custom storage logic
        return 'https://example.com/file.png'

# Register custom backend
from utils.storage_manager import register_backend
register_backend('custom', CustomStorageBackend())
```

---

## Development & Contributing

### Development Setup

#### Prerequisites
- Python 3.12+
- Git for version control
- Code editor (VS Code recommended)
- Chrome/Firefox for testing

#### Local Development Environment
```bash
# 1. Clone repository
git clone <repository-url>
cd best-of-ai-agent

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate     # Windows

# 3. Install dependencies
pip install -r requirements.txt

# 4. Install development dependencies
pip install pytest black flake8 mypy

# 5. Run application
streamlit run app.py

# 6. Run tests
pytest tests/
```

### Code Structure

#### Directory Organization
```
best-of-ai-agent/
‚îú‚îÄ‚îÄ app.py                     # Main application
‚îú‚îÄ‚îÄ extractors/                # Content extraction modules
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ url_extractor.py       # Web URL extraction
‚îÇ   ‚îî‚îÄ‚îÄ newspapers_extractor.py # Newspapers.com integration
‚îú‚îÄ‚îÄ utils/                     # Utility modules
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ batch_processor.py
‚îÇ   ‚îú‚îÄ‚îÄ document_processor.py
‚îÇ   ‚îú‚îÄ‚îÄ google_drive_manager.py
‚îÇ   ‚îú‚îÄ‚îÄ storage_manager.py
‚îÇ   ‚îî‚îÄ‚îÄ logger.py
‚îú‚îÄ‚îÄ feature-docs/             # Feature documentation
‚îú‚îÄ‚îÄ logs/                     # Application logs
‚îú‚îÄ‚îÄ local_storage/           # Local file storage
‚îú‚îÄ‚îÄ tests/                   # Unit tests
‚îú‚îÄ‚îÄ pyproject.toml          # Project configuration
‚îî‚îÄ‚îÄ README.md               # Basic project info
```

#### Coding Standards

**Python Style Guide**:
- Follow PEP 8 style guidelines
- Use type hints for function parameters and returns
- Document all functions with docstrings
- Maximum line length: 88 characters (Black formatter)

**Import Organization**:
```python
# Standard library imports
import os
import sys
from typing import List, Dict

# Third-party imports
import streamlit as st
import pandas as pd

# Local imports
from extractors.url_extractor import extract_from_url
from utils.logger import setup_logging
```

**Error Handling**:
```python
def safe_operation():
    """Example of proper error handling"""
    try:
        # Risky operation
        result = perform_operation()
        logger.info("Operation completed successfully")
        return result
    except SpecificException as e:
        logger.error(f"Specific error occurred: {e}")
        return None
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        raise
```

### Testing Guidelines

#### Unit Tests
```python
# tests/test_url_extractor.py
import pytest
from extractors.url_extractor import extract_from_url

def test_extract_valid_url():
    """Test extraction from valid URL"""
    result = extract_from_url('https://example.com/article')
    assert result['success'] is True
    assert 'title' in result
    assert 'content' in result

def test_extract_invalid_url():
    """Test extraction from invalid URL"""
    result = extract_from_url('invalid-url')
    assert result['success'] is False
    assert 'error' in result
```

#### Integration Tests
```python
# tests/test_integration.py
def test_full_processing_pipeline():
    """Test complete document processing workflow"""
    # Upload document
    # Extract URLs
    # Process articles
    # Generate outputs
    # Verify results
    pass
```

#### Running Tests
```bash
# Run all tests
pytest

# Run specific test file
pytest tests/test_url_extractor.py

# Run with coverage
pytest --cov=extractors tests/

# Run performance tests
pytest tests/test_performance.py -s
```

### Contributing Guidelines

#### Pull Request Process
1. **Fork Repository**: Create personal fork on GitHub
2. **Create Branch**: `git checkout -b feature/your-feature-name`
3. **Make Changes**: Implement your feature or bug fix
4. **Add Tests**: Include tests for new functionality
5. **Run Tests**: Ensure all tests pass
6. **Update Documentation**: Update relevant documentation
7. **Submit PR**: Create pull request with detailed description

#### Commit Message Format
```
type(scope): short description

Longer description explaining the change and why it was made.

- Bullet points for additional details
- Reference issues: Fixes #123
```

**Types**: feat, fix, docs, style, refactor, test, chore

#### Code Review Checklist
- [ ] Code follows project style guidelines
- [ ] Tests pass and coverage is maintained
- [ ] Documentation is updated
- [ ] No breaking changes without version bump
- [ ] Performance impact considered
- [ ] Security implications reviewed

### Architecture Decisions

#### Key Design Principles
1. **Modularity**: Separate concerns into distinct modules
2. **Extensibility**: Easy to add new extractors and storage backends
3. **Reliability**: Robust error handling and recovery
4. **Performance**: Efficient concurrent processing
5. **Usability**: Intuitive user interface and clear feedback

#### Technology Choices
- **Streamlit**: Rapid prototyping and user-friendly interface
- **Concurrent Processing**: ThreadPoolExecutor for I/O-bound tasks
- **Storage**: Multi-tier strategy with cloud and local fallbacks
- **Authentication**: OAuth 2.0 for secure API access
- **Image Processing**: PIL for high-quality image generation

### Future Roadmap

#### Planned Features
- **Advanced Analytics**: Processing metrics and reporting
- **Custom Templates**: User-defined newspaper layouts
- **API Endpoints**: REST API for programmatic access
- **Mobile Support**: Responsive design for mobile devices
- **Real-time Collaboration**: Multi-user document editing

#### Technical Improvements
- **Performance**: Async processing for better scalability
- **Caching**: Redis integration for improved performance
- **Database**: PostgreSQL for persistent data storage
- **Monitoring**: Application performance monitoring
- **Docker**: Containerization for easier deployment

---

## Conclusion

**Best of AI Agent** represents a comprehensive solution for article extraction, processing, and presentation. With its sophisticated architecture, user-friendly interface, and extensive feature set, it serves as a powerful tool for content aggregation and professional document generation.

This documentation provides complete coverage of the application's capabilities, from basic usage to advanced development scenarios. Whether you're a user looking to process articles or a developer interested in extending the platform, this guide contains the information needed to successfully work with the system.

For additional support, feature requests, or contributions, please refer to the project repository and follow the contribution guidelines outlined in this documentation.

---

*Last Updated: 2025-07-25*  
*Version: 1.0*  
*Documentation Status: Complete*